# -*- coding: utf-8 -*-
"""Captcha using Clustering + NN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vb0X0db14Vzysbfxc7ga4B0CryuWiwc0

## **Captcha Project using Clustering and Neural Networks**

In this project, we will only deal with simple text-based CAPTCHAs.

The idea is to first split a CAPTCHA image into individual characters, and then use neural networks to recognise each character. Specifically, we will make use of K-means clustering to help us achieve this. For simplicity, throughout this assignment, we will assume that each CAPTCHA image only contains one row of characters.

## **Part 1** NumPy

The first exercise involves completing a function to generate a coordinate grid.
"""

# Generate coordinates


import numpy as np

def generate_coordinates(height, width):
	"""
	Generates a coordinate grid.
	Args:
		height: the height of the grid
		width: the width of the grid
	Returns:
		A NumPy array of shape (height, width, 2), where grid[a, b] returns [a, b]
	"""
	# Your code goes here!
	x = np.arange(0, width).reshape(1, -1) # (1, width)
	y = np.arange(0, height).reshape(-1, 1) # (height, 1)

	x = np.broadcast_to(x, (height, width)) # (height, width)
	y = np.broadcast_to(y, (height, width)) # (height, width)

	return np.array([y, x]).transpose(1, 2, 0) # (yx, h, w) -> (h, w, yx)

"""The next part involves writing a function to initialize coordinates to be used as the centroids of the image.  Since each character in a CAPTCHA image uses approximately the same amount of space, a good estimate for initializing the centroids would be to take the middle of the y-axis and split the x-axis into equal parts.  For 5 characters, good initial centroids may look something as follows (the centroids are shown in red)

<center>

<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAh8AAADUCAYAAAA87UGPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUD0lEQVR4nO3df2zV1f3H8VdLubd17b1di72Xjl7pJllxiGOtlCtmy+bdkBkHAxY1bFZHZnAXR+kyZ+dgWTZWMhOHLijZkmGWWbs1ERxkSkjRMpJSaEedyKgYybgR7kVHem+p0rLe8/3j+/V+vVC1P27Pvbc+H8lJuOec+7nvvqHcVz79fG5zjDFGAAAAluSmuwAAAPDxQvgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVk1a+Ni2bZtmz56t/Px81dbW6vDhw5P1UgAAIIvkTMbvdvnzn/+su+++W9u3b1dtba22bt2q1tZW9fb2qqys7EOfG4/HdebMGRUVFSknJyfVpQEAgElgjFF/f7/Ky8uVm/sR5zbMJFi4cKEJBoOJx8PDw6a8vNw0NTV95HNDoZCRxGAwGAwGIwtHKBT6yPf6PKXY0NCQuru71djYmJjLzc1VIBBQR0fHFfsHBwc1ODiYeGz+70RMKBSSy+VKdXkAAGASxGIxVVRUqKio6CP3pjx8vP322xoeHpbH40ma93g8OnHixBX7m5qa9POf//yKeZfLRfgAACDLjOaSibTf7dLY2KhoNJoYoVAo3SUBAIBJlPIzHzNmzNC0adMUiUSS5iORiLxe7xX7nU6nnE5nqssAAAAZKuVnPhwOh6qrq9XW1paYi8fjamtrk9/vT/XLAQCALJPyMx+S1NDQoLq6OtXU1GjhwoXaunWrBgYGdO+9907GywEAgCwyKeHjjjvu0FtvvaVNmzYpHA7r85//vF544YUrLkIFAAAfP5PyIWMTEYvF5Ha7FY1GudsFAIAsMZb377Tf7QIAAD5eCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwCrCBwAAsIrwAQAArCJ8AAAAqwgfAADAKsIHAACwivABAACsInwAAACrCB8AAMAqwgcAALCK8AEAAKwifAAAAKsIHwAAwKoxh48DBw7o9ttvV3l5uXJycrRr166kdWOMNm3apJkzZ6qgoECBQEAnT55MVb0AACDLjTl8DAwM6IYbbtC2bdtGXP/1r3+txx9/XNu3b1dnZ6c+8YlPaMmSJbp48eKEiwUAANkvb6xPWLp0qZYuXTrimjFGW7du1U9/+lMtW7ZMkvTHP/5RHo9Hu3bt0p133jmxagEAQNZL6TUfp06dUjgcViAQSMy53W7V1taqo6NjxOcMDg4qFoslDQAAMHWlNHyEw2FJksfjSZr3eDyJtcs1NTXJ7XYnRkVFRSpLAgAAGSbtd7s0NjYqGo0mRigUSndJAABgEqU0fHi9XklSJBJJmo9EIom1yzmdTrlcrqQBAACmrpSGj8rKSnm9XrW1tSXmYrGYOjs75ff7U/lSAAAgS435bpcLFy7o9ddfTzw+deqUenp6VFJSIp/Pp/r6ev3yl7/UnDlzVFlZqY0bN6q8vFzLly9PZd0AACBLjTl8dHV16ctf/nLicUNDgySprq5OTz31lB588EENDAzovvvuU19fn26++Wa98MILys/PT13VAAAga+UYY0y6i3i/WCwmt9utaDTK9R8AAGSJsbx/p/1uFwAA8PFC+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVWMKH01NTbrxxhtVVFSksrIyLV++XL29vUl7Ll68qGAwqNLSUhUWFmrlypWKRCIpLRoAAGSvMYWP9vZ2BYNBHTp0SPv27dOlS5f0ta99TQMDA4k9GzZs0O7du9Xa2qr29nadOXNGK1asSHnhGKWcnOSB8aGPqUMvU4depgZ9tC7HGGPG++S33npLZWVlam9v1xe/+EVFo1FdffXVam5u1qpVqyRJJ06c0Ny5c9XR0aFFixZ95DFjsZjcbrei0ahcLtd4S8N7Lv9GGv9f98cbfUwdepk69DI16GNKjOX9e0LXfESjUUlSSUmJJKm7u1uXLl1SIBBI7KmqqpLP51NHR8eIxxgcHFQsFksaAABg6hp3+IjH46qvr9fixYs1b948SVI4HJbD4VBxcXHSXo/Ho3A4POJxmpqa5Ha7E6OiomK8JQEAgCww7vARDAZ17NgxtbS0TKiAxsZGRaPRxAiFQhM6Hi5jTPLA+NDH1KGXqUMvU4M+Wpc3nietW7dOe/bs0YEDBzRr1qzEvNfr1dDQkPr6+pLOfkQiEXm93hGP5XQ65XQ6x1MGAADIQmM682GM0bp167Rz507t379flZWVSevV1dWaPn262traEnO9vb06ffq0/H5/aioGAABZbUxnPoLBoJqbm/Xcc8+pqKgocR2H2+1WQUGB3G631qxZo4aGBpWUlMjlcumBBx6Q3+8f1Z0uAABg6hvTrbY5H3D/844dO3TPPfdI+t8PGfvhD3+oZ555RoODg1qyZImeeOKJD/yxy+W41RYAgOwzlvfvCX3Ox2QgfAAAkH2sfc4HAADAWBE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVYwofTz75pObPny+XyyWXyyW/36/nn38+sX7x4kUFg0GVlpaqsLBQK1euVCQSSXnRAAAge40pfMyaNUtbtmxRd3e3urq69JWvfEXLli3Tq6++KknasGGDdu/erdbWVrW3t+vMmTNasWLFpBQOAACyU44xxkzkACUlJXrkkUe0atUqXX311WpubtaqVaskSSdOnNDcuXPV0dGhRYsWjep4sVhMbrdb0WhULpdrIqUBAABLxvL+Pe5rPoaHh9XS0qKBgQH5/X51d3fr0qVLCgQCiT1VVVXy+Xzq6Oj4wOMMDg4qFoslDQAAMHWNOXy88sorKiwslNPp1Nq1a7Vz505dd911CofDcjgcKi4uTtrv8XgUDoc/8HhNTU1yu92JUVFRMeYvAgAAZI8xh4/Pfvaz6unpUWdnp+6//37V1dXp+PHj4y6gsbFR0Wg0MUKh0LiPBQAAMl/eWJ/gcDh07bXXSpKqq6t15MgRPfbYY7rjjjs0NDSkvr6+pLMfkUhEXq/3A4/ndDrldDrHXjkAAMhKE/6cj3g8rsHBQVVXV2v69Olqa2tLrPX29ur06dPy+/0TfRkAADBFjOnMR2Njo5YuXSqfz6f+/n41NzfrpZde0t69e+V2u7VmzRo1NDSopKRELpdLDzzwgPx+/6jvdAEAAFPfmMLHuXPndPfdd+vs2bNyu92aP3++9u7dq69+9auSpN/85jfKzc3VypUrNTg4qCVLluiJJ56YlMIBAEB2mvDnfKQan/MBAED2sfI5HwAAAONB+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVhE+AACAVXnpLuByxhhJUiwWS3MlAABgtN57337vffzDZFz46O/vlyRVVFSkuRIAADBW/f39crvdH7onx4wmolgUj8d15swZGWPk8/kUCoXkcrnSXVZWi8ViqqiooJcpQC9Tgz6mDr1MHXo5McYY9ff3q7y8XLm5H35VR8ad+cjNzdWsWbMSp29cLhf/CFKEXqYOvUwN+pg69DJ16OX4fdQZj/dwwSkAALCK8AEAAKzK2PDhdDr1s5/9TE6nM92lZD16mTr0MjXoY+rQy9Shl/Zk3AWnAABgasvYMx8AAGBqInwAAACrCB8AAMAqwgcAALAqY8PHtm3bNHv2bOXn56u2tlaHDx9Od0kZrampSTfeeKOKiopUVlam5cuXq7e3N2nPxYsXFQwGVVpaqsLCQq1cuVKRSCRNFWePLVu2KCcnR/X19Yk5ejl6b775pr797W+rtLRUBQUFuv7669XV1ZVYN8Zo06ZNmjlzpgoKChQIBHTy5Mk0Vpx5hoeHtXHjRlVWVqqgoECf+cxn9Itf/CLpd2jQx5EdOHBAt99+u8rLy5WTk6Ndu3YlrY+mb+fPn9fq1avlcrlUXFysNWvW6MKFCxa/iinIZKCWlhbjcDjMH/7wB/Pqq6+a733ve6a4uNhEIpF0l5axlixZYnbs2GGOHTtmenp6zNe//nXj8/nMhQsXEnvWrl1rKioqTFtbm+nq6jKLFi0yN910UxqrznyHDx82s2fPNvPnzzfr169PzNPL0Tl//ry55pprzD333GM6OzvNG2+8Yfbu3Wtef/31xJ4tW7YYt9ttdu3aZV5++WXzjW98w1RWVpp33303jZVnls2bN5vS0lKzZ88ec+rUKdPa2moKCwvNY489lthDH0f2t7/9zTz88MPm2WefNZLMzp07k9ZH07dbb73V3HDDDebQoUPm73//u7n22mvNXXfdZfkrmVoyMnwsXLjQBIPBxOPh4WFTXl5umpqa0lhVdjl37pyRZNrb240xxvT19Znp06eb1tbWxJ5//etfRpLp6OhIV5kZrb+/38yZM8fs27fPfOlLX0qED3o5ej/+8Y/NzTff/IHr8XjceL1e88gjjyTm+vr6jNPpNM8884yNErPCbbfdZr773e8mza1YscKsXr3aGEMfR+vy8DGavh0/ftxIMkeOHEnsef75501OTo558803rdU+1WTcj12GhobU3d2tQCCQmMvNzVUgEFBHR0caK8su0WhUklRSUiJJ6u7u1qVLl5L6WlVVJZ/PR18/QDAY1G233ZbUM4lejsVf//pX1dTU6Fvf+pbKysq0YMEC/f73v0+snzp1SuFwOKmXbrdbtbW19PJ9brrpJrW1tem1116TJL388ss6ePCgli5dKok+jtdo+tbR0aHi4mLV1NQk9gQCAeXm5qqzs9N6zVNFxv1iubffflvDw8PyeDxJ8x6PRydOnEhTVdklHo+rvr5eixcv1rx58yRJ4XBYDodDxcXFSXs9Ho/C4XAaqsxsLS0t+sc//qEjR45csUYvR++NN97Qk08+qYaGBv3kJz/RkSNH9IMf/EAOh0N1dXWJfo30/U4v/99DDz2kWCymqqoqTZs2TcPDw9q8ebNWr14tSfRxnEbTt3A4rLKysqT1vLw8lZSU0NsJyLjwgYkLBoM6duyYDh48mO5SslIoFNL69eu1b98+5efnp7ucrBaPx1VTU6Nf/epXkqQFCxbo2LFj2r59u+rq6tJcXfb4y1/+oqefflrNzc363Oc+p56eHtXX16u8vJw+Iitl3I9dZsyYoWnTpl1x50AkEpHX601TVdlj3bp12rNnj1588UXNmjUrMe/1ejU0NKS+vr6k/fT1St3d3Tp37py+8IUvKC8vT3l5eWpvb9fjjz+uvLw8eTweejlKM2fO1HXXXZc0N3fuXJ0+fVqSEv3i+/3D/ehHP9JDDz2kO++8U9dff72+853vaMOGDWpqapJEH8drNH3zer06d+5c0vp///tfnT9/nt5OQMaFD4fDoerqarW1tSXm4vG42tra5Pf701hZZjPGaN26ddq5c6f279+vysrKpPXq6mpNnz49qa+9vb06ffo0fb3MLbfcoldeeUU9PT2JUVNTo9WrVyf+TC9HZ/HixVfc8v3aa6/pmmuukSRVVlbK6/Um9TIWi6mzs5Nevs8777yj3Nzk/66nTZumeDwuiT6O12j65vf71dfXp+7u7sSe/fv3Kx6Pq7a21nrNU0a6r3gdSUtLi3E6neapp54yx48fN/fdd58pLi424XA43aVlrPvvv9+43W7z0ksvmbNnzybGO++8k9izdu1a4/P5zP79+01XV5fx+/3G7/ensers8f67XYyhl6N1+PBhk5eXZzZv3mxOnjxpnn76aXPVVVeZP/3pT4k9W7ZsMcXFxea5554z//znP82yZcu4RfQydXV15lOf+lTiVttnn33WzJgxwzz44IOJPfRxZP39/ebo0aPm6NGjRpJ59NFHzdGjR82///1vY8zo+nbrrbeaBQsWmM7OTnPw4EEzZ84cbrWdoIwMH8YY89vf/tb4fD7jcDjMwoULzaFDh9JdUkaTNOLYsWNHYs+7775rvv/975tPfvKT5qqrrjLf/OY3zdmzZ9NXdBa5PHzQy9HbvXu3mTdvnnE6naaqqsr87ne/S1qPx+Nm48aNxuPxGKfTaW655RbT29ubpmozUywWM+vXrzc+n8/k5+ebT3/60+bhhx82g4ODiT30cWQvvvjiiP831tXVGWNG17f//Oc/5q677jKFhYXG5XKZe++91/T396fhq5k6cox530fkAQAATLKMu+YDAABMbYQPAABgFeEDAABYRfgAAABWET4AAIBVhA8AAGAV4QMAAFhF+AAAAFYRPgAAgFWEDwAAYBXhAwAAWEX4AAAAVv0PWUL3dGtTKZAAAAAASUVORK5CYII=" alt="drawing" width="400"/>

</center>
"""

# Initialize centroids

def init_centroids(height, width, K):
	"""
	Initializes centroids.
	Args:
		height: the height of the image
		width: the width of the image
		K: the number of centroids to initialize
	Returns:
		A NumPy array of shape (K, 2) containing the coordinates of the centroids.  Each centroid is in [y, x] format.
	"""

	centroids = np.full((K, 2), fill_value=height / 2)
	centroids[:, 1] = np.arange(1, K+1) * (width / (K+1)) # (5, 2) meaning (5 centroids, xy)

	return centroids

"""## **Part 2** Text extraction

Now, it is time to separate a CAPTCHA image into background and text.  Since most CAPTCHA images contain mainly 2 colours, one for the text and one for the background, we will use 2 centroids to perform clustering on the RGB values of each pixel.  We first initialize 2 random RGB colours to be the centroids, then separate the text by finding which pixels are similar in colour to the 2 centroids.  In the end, one of the converged centroids should roughly represent the colour of the background, and the other the colour of the text.
"""

# Text extraction

def pixel_centroids_dist_1(img, centroid1, centroid2):
	"""
	Finds which pixels are closer in colour to each centroid.
	Does so by computing the SQUARED Euclidean distance between each pixel's RGB values and the RGB value of the two centroids.
	Args:
		img: a NumPy array image of shape (height, width, 3)
		centroid1: a NumPy array of shape (3,) containing the RGB values of the first centroid
		centroid2: a NumPy array of shape (3,) containing the RGB values of the second centroid
	Returns:
		a NumPy array of shape (2, height, width) containing the distances.
		Indexing the returned result by [0] should be the distance to centroid1, and indexing by [1] should be the distance to centroid2.
	"""

	dist1 = ((img - centroid1) ** 2).sum(axis=2) # (height, width)
	dist2 = ((img - centroid2) ** 2).sum(axis=2) # (height, width)

	return np.array([dist1, dist2]) # (2, height, width)

def assign_pixels_1(dist):
	"""
	For each pixel, assign it to the closest centroid.
	Args:
		dist: a NumPy array of shape (2, height, width) containing the distances to each centroid
	Returns:
		a NumPy array of shape (height, width) containing only 0s and 1s, where 0 indicates closer to centroid1, while 1 indicates closer to centroid2
	"""

	return dist.argmin(axis=0) # shape (height, width)

def recompute_centroids_1(img, cluster):
	"""
	Takes the mean RGB values to recompute the two centroids based on the current cluster membership.
	Args:
		img: a NumPy array image of shape (height, width, 3)
		cluster: a NumPy array of shape (height, width) containing only 0s and 1s, where 0 indicates closer to centroid1, while 1 indicates closer to centroid2
	Returns:
		A tuple containing the two centroids.  Each centroid is of shape (3,)
	"""

	layer1 = img[cluster == 0].mean(axis=0)
	layer2 = img[cluster == 1].mean(axis=0)

	return (layer1, layer2)

# Extracting text

RNJESUS_SEED = 122807528840384100672342137672332424407
rnjesus_generator = np.random.default_rng(seed=RNJESUS_SEED)

def extract_text(img, times=10):
	"""
	Uses clustering to find the text parts of a CAPTCHA image.  Assumes the image has only 2 main colours.
	Args:
		img: a NumPy array image of shape (height, width, 3)
		times: number of times to repeat steps 2 to 4 of the algorithm.  Don't change this!
	Returns:
		a boolean NumPy array of shape (height, width), where True indicates text, False indicates background
		Assumes text occurs less frequently than background
	"""
	# STEP 1 - initialise centroid (done for you)
	centroid1 = rnjesus_generator.integers(low=0, high=256, size=3) # [150, 172,  21]
	centroid2 = rnjesus_generator.integers(low=0, high=256, size=3) # [165, 183, 215]
	cluster = None

	for _ in range(times):


		dist = pixel_centroids_dist_1(img, centroid1, centroid2)

		cluster = assign_pixels_1(dist)

		centroid1, centroid2 = recompute_centroids_1(img, cluster)

	return cluster == 0 if (cluster == 0).sum() < (cluster == 1).sum() else cluster == 1

"""## **Part 3** Character separation"""

# Character separation

def pixel_centroids_dist_2(img, centroids):
	"""
	Finds the SQUARED Euclidean distance between each pixel and each centroid.
	Args:
		img: a NumPy array image of shape (height, width, 3)
		centroids: a NumPy array of shape (K, 2) containing the yx coordinates of each centroid.  K is the number of centroids.
	Returns:
		a NumPy array of shape (K, height, width) containing the distances.
		Indexing the returned result by [j] should be the distances from each pixel to centroid j
	"""

	height, width = img.shape[0], img.shape[1]
	K = centroids.shape[0]

	yx = centroids.reshape(K, 1, 1, 2) # (5 centroids, height, width, yx)
	coordinates = np.expand_dims(generate_coordinates(height, width), axis=0) # (1, height, width, 2)
	dist = (coordinates[:, :, :, 0] - yx[:, :, :, 0]) ** 2 + (coordinates[:, :, :, 1] - yx[:, :, :, 1]) ** 2 # (K, height, width)

	return dist

# Character separation

def assign_pixels_2(dist):
	"""
	For each pixel, assign it to the closest centroid
	Args:
		dist: a NumPy array of shape (K, height, width) containing the distances to each centroid
	Returns:
		a NumPy array of shape (height, width) containing only integers, where 0 indicates closest to centroid[0], 1 indicates closest to centroid[1], and so on.
	"""

	return dist.argmin(axis=0) # shape (height, width)

def recompute_centroids_2(img, text, cluster):
	"""
	Takes the mean location to recompute the centroids based on the current cluster membership.
	For each cluster, find all pixels corresponding to that cluster that are also text.
	Then, compute the mean x and y coordinates to determine the new centroid for that cluster.
	Args:
		img: a NumPy array image of shape (height, width, 3)
		text: a boolean NumPy array of shape (height, width), where True indicates text, and False indicates background
		cluster: a NumPy array of shape (height, width) containing only integers, where 0 indicates closest to centroid[0], 1 indicates closest to centroid[1], and so on.
	Returns:
		a NumPy array of shape (K, 2) containing the recomputed yx coordinates of each centroid.  K is the number of centroids.
	"""

	height, width = img.shape[0], img.shape[1]
	K = cluster.max() + 1

	coordinates = generate_coordinates(height, width).reshape(height, width, 1, 2) # (height, width, 1, 2) meaning (h, w, layers, yx)
	coordinates = np.broadcast_to(coordinates, (height, width, K, 2))

	cluster = cluster.reshape(height, width, 1) # (h, w, 1)
	cluster = (cluster == np.arange(0, K).reshape(1, 1, K)) # (h, w, K)
	text_reshaped = text.reshape(height, width, 1) # (h, w, 1)
	text_reshaped = np.broadcast_to(text_reshaped, (height, width, K)) # (h, w, K)

	idx = (text_reshaped & cluster).reshape(height, width, K, 1)
	idx = np.broadcast_to(idx, (height, width, K, 2))

	centroids = coordinates.mean(axis=(0, 1), where=idx)

	return centroids

def separate_characters(img, K, times=10):
	"""
	Uses clustering to separate the characters of the image.
	Args:
		img: a NumPy array image of shape (height, width, 3)
		K: the number of characters in the image
		times: number of times to repeat steps 2 to 4 of the algorithm.  Don't change this!
	Returns:
		a NumPy array of shape (height, width), where 0 indicates closest to centroid[0], 1 indicates closest to centroid[1], and so on.
		Background pixels are labelled -1.
	"""
	text = extract_text(img)
	cluster = None

	centroids = init_centroids(height=img.shape[0], width=img.shape[1], K=K) # (K, 2)

	for _ in range(times):


		dist = pixel_centroids_dist_2(img, centroids)

		cluster = assign_pixels_2(dist)

		centroids = recompute_centroids_2(img, text, cluster)

	cluster[~text] = -1

	return cluster

"""## ** Trying out the model **"""

import cv2 as cv
import matplotlib.pyplot as plt


PATH = '/content/sample_data/cap2.png' # upload your image to the folder 'sample_data' using the small folder icon on the left.
K = 5

# # load the image
img = cv.imread(PATH)
img = cv.cvtColor(img, cv.COLOR_BGR2RGB) # By default, OpenCV loads as BGR format, so we need to convert it to RGB

# # show the original image
plt.imshow(img)
plt.show()

# # show the separated image
plt.imshow(separate_characters(img, K))
plt.show()

import zipfile
from google.colab import files

# Upload file
uploaded = files.upload()

# Extract the zip file
for filename in uploaded.keys():
    if filename.endswith('.zip'):
        with zipfile.ZipFile(filename, 'r') as zip_ref:
            zip_ref.extractall()
        print(f"Extracted {filename}")

# Commented out IPython magic to ensure Python compatibility.
import cv2
import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import Model, layers, Sequential
import tensorflow.keras.layers as tfkl
from random import shuffle
# %matplotlib inline
from matplotlib import pyplot as plt

PNG_FOLDER_NAME = '/content/samples/samples'
DEFAULT_CAPTCHA_LENGTH = 5

def delete_noisy_line(image:list):
    """ Some processes to delete line on the letters

    Args:
        image (list): input image that has been loaded recently

    Returns:
        image (list): converted image to approximately delete noisy line
    """
    image = cv2.blur(image, (3, 3))
    ret, image = cv2.threshold(image, 90, 255, cv2.THRESH_BINARY)

    image = cv2.dilate(image, np.ones((3, 1), np.uint8))
    image = cv2.erode(image, np.ones((2, 2), np.uint8))

    return image


def load_images_from_folder(folder:str, shuffled_files:list):
    """ This function load images from the determined folder

    Args:
        folder (str): the name of folder
        shuffled_files (list): list of files in determined folder those are shuffled

    Returns:
        images (list): the variable to save images
        splited_labels (list): return the characters of what images show
        unique_chars (dict): the unique characters that have been found in labels
    """
    images = []
    labels = []
    unique_chars = {}

    for filename in shuffled_files:
        img = cv2.imread(os.path.join(folder,filename), cv2.IMREAD_GRAYSCALE)
        img = delete_noisy_line(img)

        # Add image with its label to `images` and `labels`
        if img is not None:
            images.append(img)
            labels.append(filename.split('.')[0])

            # Find unique characters with their number
            for char in filename.split('.')[0]:
                if char in unique_chars:
                    unique_chars[char] += 1
                else:
                    unique_chars[char] = 1

    return images, labels, unique_chars



def preprocess_data(images:np.ndarray, labels:np.ndarray, unique_chars:dict, shuffled_files:list):
    """ This function apply preprocess images and labels to convert them to proper shape

    Args:
        images (np.ndarray): The images those are saved in the type of array
        labels (np.ndarray):  the labels in shape: (sample's count,)
        unique_chars (dict): unique chars that are found in captchas
        shuffled_files (list): list of files in determined folder those are shuffled

    Returns:
        X (np.ndarray): converted images in shape: (sample's count, height, width, 1)
        y (np.ndarray): converted labels in shape: (length of each example, sample's count, unique chars)
    """
    captcha_length = len(labels[0])
    unique_chars_count = len(unique_chars)

    X = np.zeros(images[...,  np.newaxis].shape)
    y = np.zeros((captcha_length, len(labels), unique_chars_count))

    for i, img in enumerate(shuffled_files):
        # Divide cells of images by 255 to learn more quickly in neural network
        captcha_img = images[i]
        captcha_img = captcha_img/255.0

        # Reshape `images`
        captcha_img = np.reshape(captcha_img, images[0, ...,  np.newaxis].shape)
        X[i] = captcha_img

        # Reshape `labels`
        curr_name = np.zeros((captcha_length, unique_chars_count))
        for j, char in enumerate(labels[i]):
            curr_name[j, list(unique_chars.keys()).index(char)] = 1
        y[:, i] = curr_name

    return X, y

# Shuffle folders' name to unorder the files
shuffled_files = os.listdir(PNG_FOLDER_NAME)
shuffle(shuffled_files)

images, labels, unique_chars = load_images_from_folder(PNG_FOLDER_NAME, shuffled_files)
images, labels = np.array(images), np.array(labels)
images, labels = preprocess_data(images, labels, unique_chars, shuffled_files)

unique_chars_count = len(unique_chars)

print(f"images' shape is {images.shape}")
print(f"labels' shape is {labels.shape}")
print(f"unique characters count equals: {unique_chars_count}")

index = 32
plt.imshow(images[index])
plt.show()
print(f"array of label number {index} equals:\n {labels[:, index]}")

class MyModel(Model):
    def __init__(self, num_classes):
        super(MyModel, self).__init__()

        self.layer1 = Sequential([
            tfkl.Conv2D(16, activation='relu', kernel_size=5, strides=1, padding="same"),
            tfkl.MaxPool2D(pool_size=(2, 2), padding='same'),
        ])
        self.layer2 = Sequential([
            tfkl.Conv2D(32, activation='relu', kernel_size=3, strides=1, padding="same"),
            tfkl.MaxPool2D(pool_size=(2, 2), padding='same'),
        ])
        self.layer3 = Sequential([
            tfkl.Conv2D(64, activation='relu', kernel_size=3, strides=1, padding="same"),
            tfkl.MaxPool2D(pool_size=(2, 2), padding='same'),
        ])
        self.layer4 = Sequential([
            tfkl.Conv2D(128, activation='relu', kernel_size=3, strides=1, padding="same"),
            tfkl.MaxPool2D(pool_size=(2, 2), padding='same'),
        ])
        self.layer5 = Sequential([
            tfkl.Conv2D(256, activation='relu', kernel_size=3, strides=1, padding="same"),
            tfkl.MaxPool2D(pool_size=(2, 2), padding='same'),
        ])
        self.layer6 = Sequential([
            tfkl.Flatten(),
            tfkl.Dropout(0.2),
            tfkl.BatchNormalization(),
        ])
        self.layer7_1 = Sequential([
            tfkl.Dense(64, activation='relu'),
            tfkl.Dropout(0.2),
            tfkl.BatchNormalization(),
            tfkl.Dense(num_classes , activation='softmax', name='char1'),
        ])
        self.layer7_2 = Sequential([
            tfkl.Dense(64, activation='relu'),
            tfkl.Dropout(0.2),
            tfkl.BatchNormalization(),
            tfkl.Dense(num_classes , activation='softmax', name='char2'),
        ])
        self.layer7_3 = Sequential([
            tfkl.Dense(64, activation='relu'),
            tfkl.Dropout(0.2),
            tfkl.BatchNormalization(),
            tfkl.Dense(num_classes , activation='softmax', name='char3'),
        ])
        self.layer7_4 = Sequential([
            tfkl.Dense(64, activation='relu'),
            tfkl.Dropout(0.2),
            tfkl.BatchNormalization(),
            tfkl.Dense(num_classes , activation='softmax', name='char4'),
        ])
        self.layer7_5 = Sequential([
            tfkl.Dense(64, activation='relu'),
            tfkl.Dropout(0.2),
            tfkl.BatchNormalization(),
            tfkl.Dense(num_classes , activation='softmax', name='char5'),
        ])

    def call(self, inputs):
        x = self.layer1(inputs)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.layer5(x)
        x = self.layer6(x)
        output1, output2, output3, output4, output5 = self.layer7_1(x), self.layer7_2(x), self.layer7_3(x), self.layer7_4(x), self.layer7_5(x)
        output = [output1, output2, output3, output4, output5]
        return output

input = tfkl.Input(images[0].shape)
output = MyModel(unique_chars_count)(input)

model = Model(inputs=input, outputs=output)

model.summary()

model.compile(loss='categorical_crossentropy', metrics=['accuracy', 'accuracy', 'accuracy', 'accuracy', 'accuracy'], optimizer='adam')

train_size = 0.8
numTrainingSamples = int(len(images) * train_size)
numTestingSamples = int(len(images) - numTrainingSamples)

X_train = images[:numTrainingSamples]
y_train = labels[:, :numTrainingSamples]
X_test = images[numTrainingSamples:]
y_test = labels[:, numTrainingSamples:]

history = model.fit(
    X_train,
    [y_train[0], y_train[1], y_train[2], y_train[3], y_train[4]],
    epochs=30,
    batch_size=32,
    validation_data=(
        X_test,
        [y_test[0], y_test[1], y_test[2], y_test[3], y_test[4]]
    )
    # Don't specify metrics - Keras will use defaults
)

modelEvaluation = model.evaluate(X_test, [y_test[0], y_test[1], y_test[2], y_test[3], y_test[4]])

print(f'Character1 Prediction Accuracy : {modelEvaluation[6]*100} %')
print(f'Character2 Prediction Accuracy : {modelEvaluation[7]*100} %')
print(f'Character3 Prediction Accuracy : {modelEvaluation[8]*100} %')
print(f'Character4 Prediction Accuracy : {modelEvaluation[9]*100} %')
print(f'Character5 Prediction Accuracy : {modelEvaluation[10]*100} %')

def draw_loss_diagram_of_history(curr_history):
    plt.plot(curr_history.history['loss'])
    plt.plot(curr_history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

draw_loss_diagram_of_history(history)

characters = list(unique_chars.keys())
predicted_word = []
true_word = []

for i, sample in enumerate(X_test):
    sample = np.reshape(X_test[i] , images[0][np.newaxis, :].shape)
    sample = model.predict(sample)
    sample = np.reshape(sample ,(DEFAULT_CAPTCHA_LENGTH, unique_chars_count))
    predicted = ''.join([characters[np.argmax(i)] for i in sample])
    predicted_word.append(predicted)

for i in range(0, numTestingSamples):
    temp = ''.join([characters[i] for i in (np.argmax(y_test[:, i],axis=1))])
    true_word.append(temp)

predicted_word = np.array(predicted_word)
true_word = np.array(true_word)
df = pd.DataFrame({'Predicted_value': predicted_word, 'True_value': list(true_word)}, columns=['Predicted_value', 'True_value'])
print(df)

df.to_csv('result.csv', index=False)